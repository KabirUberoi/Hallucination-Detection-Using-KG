{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_trf'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokens\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Doc, Span\n\u001b[0;32m----> 3\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men_core_web_trf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m nlp\u001b[38;5;241m.\u001b[39madd_pipe(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdbpedia_spotlight\u001b[39m\u001b[38;5;124m'\u001b[39m, config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfidence\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.5\u001b[39m})\n\u001b[1;32m      5\u001b[0m preprocessing \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/spacy/__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[1;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/spacy/util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[0;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_trf'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc, Span\n",
    "nlp = spacy.load('en_core_web_trf')\n",
    "nlp.add_pipe('dbpedia_spotlight', config={'confidence': 0.5})\n",
    "preprocessing = spacy.load('en_core_web_sm')\n",
    "from fastcoref import LingMessCoref\n",
    "coref_model = LingMessCoref()\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"potsawee/wiki_bio_gpt3_hallucination\")\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import concurrent.futures\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coreference Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_cluster_spans(doc, clusters):\n",
    "    fast_clusters = []\n",
    "    for cluster in clusters:\n",
    "        new_group = []\n",
    "        for start, end in cluster:\n",
    "            span = doc.char_span(start, end)\n",
    "            if span is not None:\n",
    "                new_group.append([span.start, span.end - 1])\n",
    "        fast_clusters.append(new_group)\n",
    "    return fast_clusters\n",
    "\n",
    "def get_clusters(doc, text):\n",
    "    preds = coref_model.predict(texts=[text])\n",
    "    # print(f\"\\nThe clusters of same entities are as follows: {preds[0].get_clusters(as_strings=True)} \\n\")\n",
    "    clusters = preds[0].get_clusters(as_strings=False)\n",
    "    cluster_spans = get_cluster_spans(doc, clusters)\n",
    "    return cluster_spans\n",
    "\n",
    "def get_span_noun_indices(doc, cluster):    \n",
    "    spans = [doc[start:end+1] for start, end in cluster]\n",
    "\n",
    "    spans_pos = []\n",
    "    for span in spans:\n",
    "        pos_tags = [token.pos_ for token in span]\n",
    "        spans_pos.append(pos_tags)\n",
    "\n",
    "    noun_indices = []\n",
    "    for i, pos_list in enumerate(spans_pos):\n",
    "        if 'NOUN' in pos_list or 'PROPN' in pos_list:\n",
    "            noun_indices.append(i)\n",
    "    return noun_indices\n",
    "\n",
    "def get_cluster_head(doc, cluster, noun_indices):\n",
    "    head_idx = noun_indices[0]\n",
    "    head_start, head_end = cluster[head_idx]\n",
    "    head_span = doc[head_start:head_end+1]\n",
    "    return head_span, [head_start, head_end]\n",
    "\n",
    "def is_containing_other_spans(span, all_spans):\n",
    "    for s in all_spans:\n",
    "        if s[0] >= span[0] and s[1] <= span[1] and s != span:\n",
    "            return True  \n",
    "    return False\n",
    "\n",
    "def replacement(coref, resolved, mention_span):\n",
    "    start, end = coref\n",
    "    mention_text = mention_span.text_with_ws \n",
    "    resolved[start] = mention_text\n",
    "    for i in range(start + 1, end + 1):\n",
    "        resolved[i] = \"\"\n",
    "    return resolved\n",
    "\n",
    "def replace_corefs(document, clusters):\n",
    "    resolved = [token.text_with_ws for token in document]\n",
    "    all_spans = [span for cluster in clusters for span in cluster]\n",
    "\n",
    "    for cluster in clusters:\n",
    "        noun_indices = get_span_noun_indices(document, cluster)\n",
    "\n",
    "        if noun_indices:\n",
    "            mention_span, mention = get_cluster_head(document, cluster, noun_indices)\n",
    "        else:\n",
    "            start, end = cluster[0]\n",
    "            mention_span = document[start:end+1]\n",
    "            mention = cluster[0]\n",
    "            \n",
    "        for coref in cluster:\n",
    "            if coref != mention and not is_containing_other_spans(coref, all_spans):\n",
    "                resolved = replacement(coref, resolved, mention_span)\n",
    "\n",
    "    \n",
    "    return (\"\".join(resolved))\n",
    "\n",
    "\n",
    "def coreference_resolution(text):\n",
    "    doc = nlp(text)\n",
    "    clusters = get_clusters(doc, text) \n",
    "    answer= replace_corefs(doc, clusters) \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_three_word_name(entity):\n",
    "    return len(entity.text.split()) >= 3 and entity.label_ == \"PERSON\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_three_worded_names(text):\n",
    "    doc = preprocessing(text)\n",
    "    new_text = text\n",
    "    \n",
    "    for entity in doc.ents:\n",
    "        if is_three_word_name(entity):\n",
    "            words = entity.text.split()\n",
    "            new_name = f\"{words[0]} {words[-1]}\"\n",
    "            new_text = new_text.replace(entity.text, new_name)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text): \n",
    "    # text = replace_three_worded_names(text) \n",
    "    text = re.sub(r'[^\\w\\s.,()\\'\"\\-]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r\"(['\\\"])\\1+\", r\"\\1\", text)\n",
    "    # preprocessed_text = coreference_resolution(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link Formation from Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_based_links_subj(text):\n",
    "    final_text = coreference_resolution(text)\n",
    "    \n",
    "    doc = nlp(final_text)\n",
    "    \n",
    "    entities = list(doc.ents)\n",
    "    # print(\"Entities found by spaCy:\", entities)\n",
    "    \n",
    "    subjects = []\n",
    "    for sent in doc.sents:\n",
    "        found_subject = False\n",
    "        for token in sent:\n",
    "            print(f\"{token},{token.dep_}\")\n",
    "            if token.dep_ in ['nsubj', 'nsubjpass'] and token.ent_kb_id_:\n",
    "                subjects.append(token.ent_kb_id_)\n",
    "                found_subject = True\n",
    "                break\n",
    "        if not found_subject:\n",
    "            subjects.append(None)\n",
    "    # print(\"Subjects identified:\", subjects)\n",
    "    \n",
    "    sentence_forms = []\n",
    "    for sent in doc.sents:\n",
    "        entities_set = set()\n",
    "        entities = []\n",
    "        for ent in sent.ents:\n",
    "            if ent.kb_id_ and ent.kb_id_ not in entities_set:\n",
    "                entities.append(ent.kb_id_)\n",
    "                entities_set.add(ent.kb_id_)\n",
    "        sentence_forms.append(entities)\n",
    "    # print(\"Entities in each sentence:\", sentence_forms)\n",
    "    \n",
    "    pairs = []\n",
    "    count = 0\n",
    "    for i in range(len(sentence_forms)):\n",
    "        if subjects[i] is not None and len(sentence_forms[i]) > 1:\n",
    "            for entity in sentence_forms[i]:\n",
    "                if subjects[i] != entity:\n",
    "                    pairs.append([subjects[i], entity])\n",
    "                    count += 1\n",
    "    print(f\"The number of pairs is: {count}\\n\")\n",
    "    \n",
    "    return pairs, final_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'coreference_resolution' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSir John Russell Reynolds, 1st Baronet (22 May 1828 29 May 1896) was a British neurologist and physician. Reynolds was born in Romsey, Hampshire, as the son of John Reynolds, an independent minister, and the grandson of Dr. Henry Revell Reynolds.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m pairs,text \u001b[38;5;241m=\u001b[39m \u001b[43mget_sentence_based_links_subj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(pairs)\n",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m, in \u001b[0;36mget_sentence_based_links_subj\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_sentence_based_links_subj\u001b[39m(text):\n\u001b[0;32m----> 2\u001b[0m     final_text \u001b[38;5;241m=\u001b[39m \u001b[43mcoreference_resolution\u001b[49m(text)\n\u001b[1;32m      4\u001b[0m     doc \u001b[38;5;241m=\u001b[39m nlp(final_text)\n\u001b[1;32m      6\u001b[0m     entities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(doc\u001b[38;5;241m.\u001b[39ments)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'coreference_resolution' is not defined"
     ]
    }
   ],
   "source": [
    "text = \"Sir John Russell Reynolds, 1st Baronet (22 May 1828 29 May 1896) was a British neurologist and physician. Reynolds was born in Romsey, Hampshire, as the son of John Reynolds, an independent minister, and the grandson of Dr. Henry Revell Reynolds.\"\n",
    "\n",
    "pairs,text = get_sentence_based_links_subj(text)\n",
    "print(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_based_links_all(text):\n",
    "    final_text = coreference_resolution(text)\n",
    "    doc = nlp(final_text)\n",
    "    sentence_forms = []\n",
    "    for sent in doc.sents:\n",
    "        entities_set = set()  \n",
    "        entities = []  \n",
    "        for ent in sent.ents:\n",
    "            # print(ent)\n",
    "            if ent.kb_id_ != \"\" and ent.kb_id_ not in entities_set:\n",
    "                entities.append(ent.kb_id_)\n",
    "                entities_set.add(ent.kb_id_)\n",
    "        sentence_forms.append(entities)\n",
    "        # sentence_forms.append([ent.kb_id_ for ent in sent.ents if ent.kb_id_ != \"\"])\n",
    "    pairs = []\n",
    "    count = 0\n",
    "    for entities in sentence_forms:\n",
    "        if len(entities)>1:\n",
    "            for i in range(len(entities)):\n",
    "                for j in range(i+1,len(entities)):\n",
    "                    pairs.append([entities[i],entities[j]])\n",
    "                    count+=1\n",
    "    print(f\"The number of pairs is: {count}\\n\")\n",
    "    \n",
    "    return pairs,final_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spotlight Based System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link Making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_based_links(text):\n",
    "    final_text = coreference_resolution(text)\n",
    "    doc = nlp(final_text)\n",
    "    final_sents = [sents for sents in doc.sents]\n",
    "    # entities = list(doc.ents)\n",
    "    # print(\"Entities found by spaCy:\", entities)\n",
    "    \n",
    "    subjects = []\n",
    "    for sent in doc.sents:\n",
    "        found_subject = False\n",
    "        for token in sent:\n",
    "            # print(f\"{token},{token.dep_}\")\n",
    "            if token.dep_ in ['nsubj', 'nsubjpass'] and token.ent_kb_id_:\n",
    "                subjects.append(token.ent_kb_id_)\n",
    "                found_subject = True\n",
    "                break\n",
    "\n",
    "        if not found_subject:\n",
    "            subjects.append(None)\n",
    "    # print(\"Subjects identified:\", subjects)\n",
    "    \n",
    "    sentence_forms = []\n",
    "    for sent in doc.sents:\n",
    "        entities = []\n",
    "        for ent in sent.ents:\n",
    "            if ent.kb_id_:\n",
    "                entities.append(ent.kb_id_)\n",
    "        sentence_forms.append(entities)\n",
    "    # print(\"Entities in each sentence:\", sentence_forms)\n",
    "    \n",
    "    pairs = []\n",
    "    count = 0\n",
    "    for i in range(len(sentence_forms)):\n",
    "        tmp_storage = []\n",
    "        if subjects[i] is not None and len(sentence_forms[i]) > 1:\n",
    "            for entity in sentence_forms[i]:\n",
    "                if subjects[i] != entity:\n",
    "                    tmp_storage.append([subjects[i], entity])\n",
    "                    count += 1\n",
    "        pairs.append(tmp_storage)\n",
    "    print(f\"The number of pairs is: {count}\\n\")\n",
    "    \n",
    "    return pairs, final_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/24/2024 12:30:53 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86f7832cfd43472ca2bad01473e75aee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/24/2024 12:30:54 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82a6f216cb954a2695cb7223fee3c337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of pairs is: 14\n",
      "\n",
      "([[['http://dbpedia.org/resource/Wilhelm_Windelband', 'http://dbpedia.org/resource/Philosopher'], ['http://dbpedia.org/resource/Wilhelm_Windelband', 'http://dbpedia.org/resource/Neo-Kantianism']], [['http://dbpedia.org/resource/Wilhelm_Windelband', 'http://dbpedia.org/resource/Nomothetic_and_idiographic'], ['http://dbpedia.org/resource/Wilhelm_Windelband', 'http://dbpedia.org/resource/Nomothetic_and_idiographic']], [], [['http://dbpedia.org/resource/Wilhelm_Windelband', 'http://dbpedia.org/resource/Neo-Kantianism'], ['http://dbpedia.org/resource/Wilhelm_Windelband', 'http://dbpedia.org/resource/Immanuel_Kant'], ['http://dbpedia.org/resource/Wilhelm_Windelband', 'http://dbpedia.org/resource/Immanuel_Kant']], [['http://dbpedia.org/resource/Wilhelm_Windelband', 'http://dbpedia.org/resource/Positivism'], ['http://dbpedia.org/resource/Wilhelm_Windelband', 'http://dbpedia.org/resource/Philosophy']], [], [['http://dbpedia.org/resource/Wilhelm_Windelband', 'http://dbpedia.org/resource/Immanuel_Kant'], ['http://dbpedia.org/resource/Wilhelm_Windelband', 'http://dbpedia.org/resource/Georg_Wilhelm_Friedrich_Hegel'], ['http://dbpedia.org/resource/Wilhelm_Windelband', 'http://dbpedia.org/resource/Johann_Bernoulli'], ['http://dbpedia.org/resource/Wilhelm_Windelband', 'http://dbpedia.org/resource/Johann_Friedrich_Herbart'], ['http://dbpedia.org/resource/Wilhelm_Windelband', 'http://dbpedia.org/resource/Hermann_Lotze']], [], []], [Wilhelm Windelband (May 11, 1848 - October 22, 1915) was a German philosopher of the Baden School., Wilhelm Windelband is now mainly remembered for the terms \"nomothetic\" and \"idiographic\", which Wilhelm Windelband introduced., the terms \"nomothetic\" and \"idiographic\", which Wilhelm Windelband introducedhave currency in psychology and other areas, though not necessarily in line with Wilhelm Windelband original meanings., Wilhelm Windelband was a Neo-Kantian who protested other Neo-Kantians of Wilhelm Windelband time and maintained that \"to understand Kant rightly means to go beyond Kant \"., Against Wilhelm Windelband positivist contemporaries, Wilhelm Windelband argued that philosophy should engage in humanistic dialogue with the natural sciences rather than uncritically appropriating the natural sciences methodologies., Wilhelm Windelband interests in psychology and cultural sciences represented an opposition to psychologism and historicism schools by a critical philosophic system., Wilhelm Windelband relied in Wilhelm Windelband effort to reach beyond Kant on such philosophers as Georg Wilhelm Friedrich Hegel, Johann Friedrich Herbart, and Hermann Lotze., Closely associated with Wilhelm Windelband was Heinrich Rickert., Wilhelm Windelband disciples were not only noted philosophers, but sociologists like Max Weber and theologians like Ernst Troeltsch and Albert Schweitzer.])\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Wilhelm Windelband (May 11, 1848 - October 22, 1915) was a German philosopher of the Baden School. Wilhelm Windelband is now mainly remembered for the terms \"nomothetic\" and \"idiographic\", which Wilhelm Windelband introduced. the terms \"nomothetic\" and \"idiographic\", which he introducedhave currency in psychology and other areas, though not necessarily in line with Wilhelm Windelband original meanings. Wilhelm Windelband was a Neo-Kantian who protested other Neo-Kantians of Wilhelm Windelband time and maintained that \"to understand Kant rightly means to go beyond Kant \". Against Wilhelm Windelband positivist contemporaries, Wilhelm Windelband argued that philosophy should engage in humanistic dialogue with the natural sciences rather than uncritically appropriating the natural sciences methodologies. Wilhelm Windelband interests in psychology and cultural sciences represented an opposition to psychologism and historicism schools by a critical philosophic system. Wilhelm Windelband relied in Wilhelm Windelband effort to reach beyond Kant on such philosophers as Georg Wilhelm Friedrich Hegel, Johann Friedrich Herbart, and Hermann Lotze. Closely associated with Wilhelm Windelband was Heinrich Rickert. Wilhelm Windelband disciples were not only noted philosophers, but sociologists like Max Weber and theologians like Ernst Troeltsch and Albert Schweitzer.\"\"\"\n",
    "print(get_sentence_based_links(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_direct_link(source_target):\n",
    "    source_uri, target_uri = source_target\n",
    "    sparql = SPARQLWrapper(\"https://dbpedia.org/sparql\")\n",
    "    sparql.setMethod('POST')  \n",
    "    \n",
    "    query_source_to_target = f\"\"\"\n",
    "    ASK WHERE {{\n",
    "      <{source_uri}> ?p <{target_uri}> .\n",
    "    }}\n",
    "    \"\"\"\n",
    "    sparql.setQuery(query_source_to_target)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "\n",
    "    try:\n",
    "        result_source_to_target = sparql.query().convert()\n",
    "        has_link_source_to_target = result_source_to_target['boolean']\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying {source_uri} -> {target_uri}: {e}\")\n",
    "        has_link_source_to_target = False\n",
    "\n",
    "    # Query from target to source\n",
    "    query_target_to_source = f\"\"\"\n",
    "    ASK WHERE {{\n",
    "      <{target_uri}> ?p <{source_uri}> .\n",
    "    }}\n",
    "    \"\"\"\n",
    "    sparql.setQuery(query_target_to_source)\n",
    "\n",
    "    try:\n",
    "        result_target_to_source = sparql.query().convert()\n",
    "        has_link_target_to_source = result_target_to_source['boolean']\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying {target_uri} -> {source_uri}: {e}\")\n",
    "        has_link_target_to_source = False\n",
    "\n",
    "    # Combine the results\n",
    "    has_link = has_link_source_to_target or has_link_target_to_source\n",
    "    return source_uri, target_uri, has_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_check_direct_link(source_target):\n",
    "    source_uri, target_uri = source_target\n",
    "    sparql = SPARQLWrapper(\"https://dbpedia.org/sparql\")\n",
    "    sparql.setMethod('POST')  \n",
    "    \n",
    "    query_source_to_target = f\"\"\"\n",
    "    ASK WHERE {{\n",
    "      <{source_uri}> ?p <{target_uri}> .\n",
    "    }}\n",
    "    \"\"\"\n",
    "    sparql.setQuery(query_source_to_target)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "\n",
    "    try:\n",
    "        result_source_to_target = sparql.query().convert()\n",
    "        has_link_source_to_target = result_source_to_target['boolean']\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying {source_uri} -> {target_uri}: {e}\")\n",
    "        has_link_source_to_target = False\n",
    "\n",
    "    # Query from target to source\n",
    "    query_target_to_source = f\"\"\"\n",
    "    ASK WHERE {{\n",
    "      <{target_uri}> ?p <{source_uri}> .\n",
    "    }}\n",
    "    \"\"\"\n",
    "    sparql.setQuery(query_target_to_source)\n",
    "\n",
    "    try:\n",
    "        result_target_to_source = sparql.query().convert()\n",
    "        has_link_target_to_source = result_target_to_source['boolean']\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying {target_uri} -> {source_uri}: {e}\")\n",
    "        has_link_target_to_source = False\n",
    "\n",
    "    # Combine the results\n",
    "    has_link = has_link_source_to_target or has_link_target_to_source\n",
    "    return source_uri, target_uri, has_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(check_direct_link([\"http://dbpedia.org/resource/John_Russell_Reynolds\",\"http://dbpedia.org/resource/Judge\"])[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def making_the_linear_version(text):\n",
    "    pairs,final_sents = get_sentence_based_links(text)\n",
    "    \n",
    "    if not pairs:\n",
    "        print(\"No entity pairs found here.\")\n",
    "        return 0,[],final_sents\n",
    "    \n",
    "    fractions = []\n",
    "    pair_and_values = []\n",
    "    \n",
    "    for sent_sets in pairs:\n",
    "        score = 0\n",
    "        for pair in sent_sets:\n",
    "            if check_direct_link(pair):\n",
    "                score+=1\n",
    "                pair_and_values.append([pair[0],pair[1],1])\n",
    "            else:\n",
    "                pair_and_values.append([pair[0],pair[1],0])\n",
    "        if len(sent_sets)==0:\n",
    "            fractions.append(-1)\n",
    "        else:\n",
    "            fractions.append(score/len(sent_sets))\n",
    "    \n",
    "    return fractions, pair_and_values, final_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentence_set(index_and_sent_sets):\n",
    "    index, sent_sets = index_and_sent_sets\n",
    "    score = 0\n",
    "    local_pair_and_values = []\n",
    "    for pair in sent_sets:\n",
    "        if check_direct_link(pair)[2]:\n",
    "            score += 1\n",
    "            local_pair_and_values.append([pair[0], pair[1], 1])\n",
    "        else:\n",
    "            local_pair_and_values.append([pair[0], pair[1], 0])\n",
    "    fraction = score / len(sent_sets) if len(sent_sets) > 0 else -1\n",
    "    return index, fraction, local_pair_and_values\n",
    "\n",
    "def making_the_parallel_version(text):\n",
    "    pairs, final_sents = get_sentence_based_links(text)\n",
    "\n",
    "    if not pairs:\n",
    "        print(\"No entity pairs found here.\")\n",
    "        return 0, [], final_sents\n",
    "\n",
    "    fractions = [None] * len(pairs)\n",
    "    pair_and_values = [None] * len(pairs)\n",
    "\n",
    "    # Set max_workers to 50\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        # Process each sentence set in parallel\n",
    "        results = list(executor.map(process_sentence_set, enumerate(pairs)))\n",
    "\n",
    "    # Place results back into the correct order\n",
    "    for index, fraction, local_pair_and_values in results:\n",
    "        fractions[index] = fraction\n",
    "        pair_and_values[index] = local_pair_and_values\n",
    "\n",
    "    # Flatten pair_and_values list\n",
    "    flat_pair_and_values = [item for sublist in pair_and_values for item in sublist]\n",
    "\n",
    "    return fractions, flat_pair_and_values, final_sents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_direct_links_and_fraction(text, num_workers=50):\n",
    "    pairs,final_text = get_sentence_based_links(text)\n",
    "    \n",
    "    if not pairs:\n",
    "        print(\"No entity pairs found.\")\n",
    "        return 0,[],final_text\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        futures = {executor.submit(check_direct_link, pair): pair for pair in pairs}\n",
    "        \n",
    "        results = []\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results.append(result)\n",
    "            except Exception as e:\n",
    "                source_target = futures[future]\n",
    "                print(f\"Error processing pair {source_target}: {e}\")\n",
    "    \n",
    "    pairs_and_values = [[a, b, 1] if has_link else [a, b, 0] for a, b, has_link in results]\n",
    "    direct_links_count = sum(entity[2] for entity in pairs_and_values)\n",
    "    \n",
    "    fraction = 0\n",
    "    if(len(pairs)>0):\n",
    "        fraction = direct_links_count/len(pairs)\n",
    "    \n",
    "    return fraction, pairs_and_values, final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/24/2024 12:31:15 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de8a35d7ed354f3fbd764c11838f5b7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/24/2024 12:31:15 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dd0bcf8acc8403fb015753291e600aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of pairs is: 14\n",
      "\n",
      "([1.0, 1.0, -1, 1.0, 0.0, -1, 0.8, -1, -1], [['http://dbpedia.org/resource/Wilhelm_Windelband', 'http://dbpedia.org/resource/Philosopher', 1], ['http://dbpedia.org/resource/Wilhelm_Windelband', 'http://dbpedia.org/resource/Neo-Kantianism', 1], ['http://dbpedia.org/resource/Wilhelm_Windelband', 'http://dbpedia.org/resource/Nomothetic_and_idiographic', 1], ['http://dbpedia.org/resource/Wilhelm_Windelband', 'http://dbpedia.org/resource/Nomothetic_and_idiographic', 1], ['http://dbpedia.org/resource/Wilhelm_Windelband', 'http://dbpedia.org/resource/Neo-Kantianism', 1], ['http://dbpedia.org/resource/Wilhelm_Windelband', 'http://dbpedia.org/resource/Immanuel_Kant', 1], ['http://dbpedia.org/resource/Wilhelm_Windelband', 'http://dbpedia.org/resource/Immanuel_Kant', 1], ['http://dbpedia.org/resource/Wilhelm_Windelband', 'http://dbpedia.org/resource/Positivism', 0], ['http://dbpedia.org/resource/Wilhelm_Windelband', 'http://dbpedia.org/resource/Philosophy', 0], ['http://dbpedia.org/resource/Wilhelm_Windelband', 'http://dbpedia.org/resource/Immanuel_Kant', 1], ['http://dbpedia.org/resource/Wilhelm_Windelband', 'http://dbpedia.org/resource/Georg_Wilhelm_Friedrich_Hegel', 1], ['http://dbpedia.org/resource/Wilhelm_Windelband', 'http://dbpedia.org/resource/Johann_Bernoulli', 0], ['http://dbpedia.org/resource/Wilhelm_Windelband', 'http://dbpedia.org/resource/Johann_Friedrich_Herbart', 1], ['http://dbpedia.org/resource/Wilhelm_Windelband', 'http://dbpedia.org/resource/Hermann_Lotze', 1]], [Wilhelm Windelband (May 11, 1848 - October 22, 1915) was a German philosopher of the Baden School., Wilhelm Windelband is now mainly remembered for the terms \"nomothetic\" and \"idiographic\", which Wilhelm Windelband introduced., the terms \"nomothetic\" and \"idiographic\", which Wilhelm Windelband introducedhave currency in psychology and other areas, though not necessarily in line with Wilhelm Windelband original meanings., Wilhelm Windelband was a Neo-Kantian who protested other Neo-Kantians of Wilhelm Windelband time and maintained that \"to understand Kant rightly means to go beyond Kant \"., Against Wilhelm Windelband positivist contemporaries, Wilhelm Windelband argued that philosophy should engage in humanistic dialogue with the natural sciences rather than uncritically appropriating the natural sciences methodologies., Wilhelm Windelband interests in psychology and cultural sciences represented an opposition to psychologism and historicism schools by a critical philosophic system., Wilhelm Windelband relied in Wilhelm Windelband effort to reach beyond Kant on such philosophers as Georg Wilhelm Friedrich Hegel, Johann Friedrich Herbart, and Hermann Lotze., Closely associated with Wilhelm Windelband was Heinrich Rickert., Wilhelm Windelband disciples were not only noted philosophers, but sociologists like Max Weber and theologians like Ernst Troeltsch and Albert Schweitzer.])\n"
     ]
    }
   ],
   "source": [
    "# #test =  [\"John Russell Reynolds (1820â€“1876) was an English lawyer, judge, and author.\", \"He was born in London, the son of a barrister, and was educated at Eton College and Trinity College, Cambridge.\", \"He was called to the bar in 1845, and became a Queen's Counsel in 1859.\", \"He was appointed a judge of the Court of Common Pleas in 1867, and was knighted in 1871.\", \"Reynolds was a prolific author, writing on a wide range of topics.\", \"He wrote several books on legal topics, including The Law of Libel and Slander (1863), The Law of Copyright (1865), and The Law of Patents for Inventions (1868).\", \"He also wrote on a variety of other topics, including history, biography, and literature.\", \"He was a frequent contributor to the Saturday Review, and wrote several books on Shakespeare, including The Mystery of William Shakespeare (1848) and The Authorship of Shakespeare (1875).\", \"He also wrote a biography of the poet John Keats (1848).\" ]\n",
    "# test = [ \"Gordon David Strachan (born 9 February 1957) is a Scottish football manager and former player.\", \"He is the manager of the Scotland national team.\", \"Strachan played for Dundee, Aberdeen, Manchester United, Leeds United and Coventry City, as well as the Scotland national team.\", \"He has also managed Coventry City, Southampton, Celtic and Middlesbrough.\", \"Strachan began his managerial career at Coventry City in 1996, leading them to the 1997 FA Cup Final, where they lost to Tottenham Hotspur.\", \"He then moved to Southampton in 2001, where he guided them to the 2003 FA Cup Final, which they lost to Arsenal.\", \"In 2005, he was appointed manager of Celtic, where he won three consecutive Scottish Premier League titles and the Scottish League Cup twice.\", \"He left Celtic in 2009 and was appointed manager of Middlesbrough in October 2010.\", \"He left Middlesbrough in October 2013.\", \"In January 2013, Strachan was appointed manager of the Scotland national team.\", \"He has since led Scotland to the UEFA Euro 2016 qualifying playoffs, where they were eliminated by eventual finalists, and to the 2018 FIFA World Cup\" ]\n",
    "# text = \"\"\n",
    "# for i in test:\n",
    "#    text += \" \" + i\n",
    "# print(text)\n",
    "print(making_the_parallel_version(text))\n",
    "\n",
    "# print(end_time - start_time)\n",
    "\n",
    "# def number_of_direct_links_in_dbpedia(text):\n",
    "#     pairs = get_sentence_based_links(text)\n",
    "#     number_of_pairs = len(pairs)\n",
    "#     existing_links = 0\n",
    "    \n",
    "#     for i in tqdm(range(len(pairs)), desc=\"Processing entries\", unit=\"entry\"):\n",
    "#         if(check_direct_link(pairs[i][0], pairs[i][1])):\n",
    "#             existing_links= existing_links+1\n",
    "    \n",
    "#     fraction = -1\n",
    "#     if(number_of_pairs>0):\n",
    "#         fraction = existing_links/number_of_pairs\n",
    "#     print(f\"The fraction of correct links = {fraction}\")\n",
    "#     return fraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IO Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_i_th_entry_in_database(i):\n",
    "    sentences = dataset[\"evaluation\"][i][\"gpt3_text\"]\n",
    "    ground_truth = dataset[\"evaluation\"][i][\"wiki_bio_text\"]\n",
    "    annotation = dataset[\"evaluation\"][i][\"annotation\"]\n",
    "\n",
    "    ground_truth_pairs = get_sentence_based_links(ground_truth)\n",
    "    sentence_pairs = get_sentence_based_links(sentences)\n",
    "    \n",
    "    fraction = 0; \n",
    "    count = len(sentence_pairs)\n",
    "    match = 0\n",
    "    for pair in sentence_pairs:\n",
    "        temp = [pair[1],pair[0]]\n",
    "        if pair in ground_truth_pairs or temp in ground_truth_pairs:\n",
    "            match+=1\n",
    "    if(count!=0):\n",
    "        fraction=match/count\n",
    "    return sentences, ground_truth,fraction,annotation,sentence_pairs,ground_truth_pairs\n",
    "  \n",
    "def write_entries_to_files(entries, folder_name=\"Self_GPT_Testing\"):\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "    \n",
    "    for i, entry in enumerate(entries):\n",
    "        filename = os.path.join(folder_name, f\"entry_{i+1}.txt\")\n",
    "        with open(filename, 'w') as file:\n",
    "            file.write(\"#############GROUND_PAIRS############\\n\\n\")\n",
    "            for x in entry[5]:\n",
    "                file.write(f\"{x[0]} and {x[1]}\\n\\n\")\n",
    "            file.write(\"#############SENTENCE_PAIRS############\\n\\n\")\n",
    "            for x in entry[4]:\n",
    "                file.write(f\"{x[0]} and {x[1]}\\n\\n\")\n",
    "            file.write(\"%%%%%%%%%%%%%%%%%%%%SENTENCES%%%%%%%%%%%%%%%%%\\n\")\n",
    "            file.write(entry[0])\n",
    "            file.write(\"\\n\\n\")\n",
    "            file.write(\"%%%%%%%%%%%%%%%%%%%%GROUND_TRUTH%%%%%%%%%%%%%%%\\n\")\n",
    "            file.write(entry[1])\n",
    "            file.write(\"\\n\\n\")\n",
    "            file.write(\"%%%%%%%%%%%%%%%%%%%%FRACTIONS%%%%%%%%%%%%%%%%%%\\n\")\n",
    "            file.write(f\"{entry[2]}\")\n",
    "            file.write(\"\\n\\n\")\n",
    "            file.write(\"%%%%%%%%%%%%%%%%%%%%ANNOTATIONS%%%%%%%%%%%%%%%%\\n\")\n",
    "            for i in entry[3]:\n",
    "                file.write(f\"{i} \")\n",
    "            file.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WikiData Link Formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import requests\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to link entities to Wikidata/Wikipedia\n",
    "def link_entity(entity):\n",
    "    url = f\"https://www.wikidata.org/w/api.php?action=wbsearchentities&search={entity}&language=en&format=json\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an HTTPError for bad responses (4xx and 5xx)\n",
    "        data = response.json()\n",
    "        if 'search' in data and data['search']:\n",
    "            entity_id = data['search'][0]['id']\n",
    "            entity_name = data['search'][0].get('label', 'No name available')\n",
    "            entity_description = data['search'][0].get('description', 'No description available')\n",
    "            return entity_id, entity_name, entity_description\n",
    "        else:\n",
    "            return None, None, None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "        return None, None, None\n",
    "    except ValueError as e:\n",
    "        print(f\"Failed to parse JSON: {e}\")\n",
    "        return None, None, None\n",
    "    \n",
    "# Function to verify direct links in Wikidata\n",
    "def verify_link(entity1, entity2):\n",
    "    url = f\"https://query.wikidata.org/sparql?query=SELECT ?item WHERE {{ wd:{entity1} ?p wd:{entity2} }}&format=json\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an HTTPError for bad responses (4xx and 5xx)\n",
    "        data = response.json()\n",
    "        return bool(data.get('results', {}).get('bindings', []))\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "        return False\n",
    "    except ValueError as e:\n",
    "        print(f\"Failed to parse JSON: {e}\")\n",
    "        return False\n",
    "\n",
    "# Function to analyze text\n",
    "def analyze_text(text):\n",
    "    doc = nlp(text)\n",
    "    entities = {ent.text: link_entity(ent.text) for ent in doc.ents if link_entity(ent.text) is not None}\n",
    "    links = []\n",
    "    direct_links = 0\n",
    "    direct_link_bools = []\n",
    "    sentences = list(doc.sents)\n",
    "    for sent in sentences:\n",
    "        sent_entities = [ent.text for ent in sent.ents]\n",
    "        for i in range(len(sent_entities)):\n",
    "            for j in range(i + 1, len(sent_entities)):\n",
    "                entity1 = entities[sent_entities[i]]\n",
    "                entity2 = entities[sent_entities[j]]\n",
    "                if entity1 and entity2:\n",
    "                    links.append((entity1, entity2))\n",
    "                    # is_direct = verify_link(entity1[0], entity2[0])\n",
    "                    is_direct = 1\n",
    "                    direct_link_bools.append(1 if is_direct else 0)\n",
    "                    if is_direct:\n",
    "                        direct_links += 1\n",
    "    ratio = direct_links / len(links) if links else 0\n",
    "    return ratio, links, direct_link_bools\n",
    "\n",
    "# Example usage:\n",
    "text = \"John F. Kennedy was the 35th President of the United States. He was assassinated in 1963.\"\n",
    "ratio, links, direct_link_bools = analyze_text(text)\n",
    "print(f\"Ratio: {ratio}\")\n",
    "print(f\"Links: {links}\")\n",
    "print(f\"Direct Link Bools: {direct_link_bools}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_scores = []\n",
    "incorrect_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "accurate = []\n",
    "minor_inaccurate = []\n",
    "major_inaccurate = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "898eb0a27eb6463f86ce4fca0c9e1971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing entries:   0%|          | 0/100 [00:00<?, ?entry/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/24/2024 12:35:06 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b158f5d849e4d3e9a7b1429e597c96a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/24/2024 12:35:07 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00623960624f48508e444c6f61da0a21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of pairs is: 12\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/24/2024 12:35:16 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e2b0911ad9f4dedb8bd7bf8e2432f4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/24/2024 12:35:17 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58b6163326af42729551a3403d966743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of pairs is: 40\n",
      "\n",
      "The ground truth scores are: 0.3134920634920635\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/24/2024 12:35:30 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1963527dec0a4287b4c092a58664c8c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/24/2024 12:35:30 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0a2bafd640c43d9adf3a88ac46356a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of pairs is: 16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/24/2024 12:35:40 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63a8007fdf8b44ea98a967b3476f1e36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/24/2024 12:35:40 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "518fa86f2b4b48a294036c79b7929e4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of pairs is: 29\n",
      "\n",
      "The ground truth scores are: 0.3365800865800866\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/24/2024 12:35:59 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b174aa861724fd4b711f31b2193b932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/24/2024 12:35:59 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdf378f5044b464486dd8260f5da7414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of pairs is: 22\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/24/2024 12:36:16 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "513d6dd28c3d4ef7b9adcf917752ed17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/24/2024 12:36:16 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a5747cae84b464799d484200513b4ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of pairs is: 65\n",
      "\n",
      "The ground truth scores are: 0.3624603174603175\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/24/2024 12:36:39 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "852e701d1bc1459b9e7ffee7aa8ee96c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/24/2024 12:36:39 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8865ba6449234d76b7b19ceabf35a99f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of pairs is: 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/24/2024 12:36:46 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "694b1a4272f9415182bf092e26d2b72c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/24/2024 12:36:46 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c326003725463fb5bb1786ae154ee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of pairs is: 27\n",
      "\n",
      "The ground truth scores are: 0.027777777777777776\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/24/2024 12:37:05 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a1a438c811a4b56bfb4c0355e102977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/24/2024 12:37:05 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bc5ba72e5884e3f89f94d39001ad69d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of pairs is: 9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/24/2024 12:37:14 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e48c03cd312442dba3e451922278cd21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/24/2024 12:37:15 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cdddda7ed2b4793a35578963a980ebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of pairs is: 0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/24/2024 12:37:18 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ed7b3437cd246b283c5c2dac3eaff25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/24/2024 12:37:18 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "683c4ebe16374245bd3e7f490db2f3ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of pairs is: 21\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/24/2024 12:37:29 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5de5adf15c7d498ea5f2581a39b8e3db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/24/2024 12:37:29 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69be6e4bff9c48c4aa5adbaeaa039738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of pairs is: 9\n",
      "\n",
      "The ground truth scores are: 0.1111111111111111\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/24/2024 12:37:48 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f4290522f44ff780df8e260014a824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/24/2024 12:37:48 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01b7e74f98a04522aec769c4cddd1342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of pairs is: 21\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/24/2024 12:38:02 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47db8490b68a44c38a887711673d1429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/24/2024 12:38:02 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7f5c3083c034f129eae7c5f5a2eff5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of pairs is: 17\n",
      "\n",
      "The ground truth scores are: 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/24/2024 12:38:16 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a94f90544f24aa4aaecd8814054afa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/24/2024 12:38:16 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b86596f6d784d1ba77bf83b7ec548b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of pairs is: 11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "folder_name = \"Output\"\n",
    "\n",
    "if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "for i in tqdm(range(0,100), desc=\"Processing entries\", unit=\"entry\"):\n",
    "    list_of_sentences = (dataset[\"evaluation\"][i][\"gpt3_sentences\"])\n",
    "    sentences = ''''''\n",
    "    for s in list_of_sentences:\n",
    "        tmp = preprocess_text(s)\n",
    "        if tmp[-1]!='.':\n",
    "            tmp+='.'\n",
    "        sentences = sentences + tmp + \"\\n\" \n",
    "    ground_truth_doc = preprocessing(dataset[\"evaluation\"][i][\"wiki_bio_text\"])\n",
    "    ground_truth = ''''''\n",
    "    for sent in ground_truth_doc.sents:\n",
    "        temp = preprocess_text(sent.text)\n",
    "        if temp[-1]!='.':\n",
    "            temp+='.'\n",
    "        ground_truth = ground_truth + temp + \"\\n\" \n",
    "    annotation = dataset[\"evaluation\"][i][\"annotation\"]\n",
    "    \n",
    "    sentences_scores, sentence_pairs_and_values , sentence_coref_sents= making_the_parallel_version(sentences)\n",
    "    ground_truth_scores, ground_pairs_and_values, ground_coref_sents = making_the_parallel_version(ground_truth)\n",
    "        \n",
    "    filename = os.path.join(folder_name, f\"entry_{i+1}.txt\")\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(\"#############SENTENCE_PAIRS############\\n\\n\")\n",
    "        for x in range(len(sentence_pairs_and_values)):\n",
    "            file.write(f\"{sentence_pairs_and_values[x][0]} and {sentence_pairs_and_values[x][1]} and the value is : {sentence_pairs_and_values[x][2]}\\n\")\n",
    "        file.write(\"\\n\")\n",
    "        file.write(\"#############GROUND_PAIRS############\\n\\n\")\n",
    "        for x in range(len(ground_pairs_and_values)):\n",
    "            file.write(f\"{ground_pairs_and_values[x][0]} and {ground_pairs_and_values[x][1]} and the value is : {ground_pairs_and_values[x][2]}\\n\")\n",
    "        file.write(\"\\n\\n\")\n",
    "        file.write(\"%%%%%%%%%%%%%%%%%%%%SENTENCES%%%%%%%%%%%%%%%%%\\n\")\n",
    "        file.write(f\"Sentences : \\n\\n{sentences}\\n\\n\")\n",
    "        file.write(f\"Coref Resolved: \\n\\n\")\n",
    "        for y,x in enumerate(sentence_coref_sents):\n",
    "            file.write(f\"{y}. {x}\")\n",
    "        file.write(\"\\n\")\n",
    "        file.write(\"\\n\")\n",
    "        file.write(\"%%%%%%%%%%%%%%%%%%%%GROUND_TRUTH%%%%%%%%%%%%%%%\\n\")\n",
    "        file.write(f\"Ground Truth : \\n\\n{ground_truth} \\n\\n\")\n",
    "        file.write(f\"Coref Resolved: \\n\\n\")\n",
    "        for y,x in enumerate(ground_coref_sents):\n",
    "            file.write(f\"{y}. {x}\")\n",
    "        file.write(\"\\n\")\n",
    "        file.write(\"%%%%%%%%%%%%%%%%%%%%FRACTIONS%%%%%%%%%%%%%%%%%%\\n\")\n",
    "        file.write(f\"Value for sentences is : \\n\")\n",
    "        for x in sentences_scores:\n",
    "            file.write(f\"{x} \")\n",
    "        file.write(\"\\n\\n\")\n",
    "        file.write(f\"Value for ground truth is : \\n\")\n",
    "        for x in ground_truth_scores:\n",
    "            file.write(f\"{x} \")\n",
    "        file.write(\"\\n\\n\")\n",
    "        file.write(\"%%%%%%%%%%%%%%%%%%%%ANNOTATIONS%%%%%%%%%%%%%%%%\\n\")\n",
    "        for x in annotation:\n",
    "            file.write(f\"{x} \")\n",
    "        file.write(\"\\n\\n\")\n",
    "        \n",
    "    suma = 0\n",
    "    leng = 0\n",
    "    for x in ground_truth_scores:\n",
    "        if x!=-1:\n",
    "            suma+=x\n",
    "            leng+=1\n",
    "    \n",
    "    if leng>0:\n",
    "        print(f\"The ground truth scores are: {suma/leng}\\n\")\n",
    "\n",
    "    if (sum(ground_truth_scores)/len(ground_truth_scores))>0.1:\n",
    "        if len(sentences_scores)!=len(annotation):\n",
    "            continue\n",
    "        for scores in ground_truth_scores:\n",
    "            if scores!=-1:\n",
    "                accurate.append(scores)\n",
    "        for t,score in enumerate(sentences_scores):\n",
    "            if score==-1:\n",
    "                continue\n",
    "            if annotation[t-1]==\"accurate\":\n",
    "                accurate.append(score) \n",
    "            elif annotation[t-1]==\"minor_inaccurate\":\n",
    "                minor_inaccurate.append(score)\n",
    "            elif annotation[t-1]==\"major_inaccurate\":\n",
    "                major_inaccurate.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5744421199442121"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(accurate)/len(accurate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "239"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(accurate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39325912183055034"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(minor_inaccurate)/len(minor_inaccurate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(minor_inaccurate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28456790123456793"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(major_inaccurate)/len(major_inaccurate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(major_inaccurate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wiki Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checking = []\n",
    "correct_scores = []\n",
    "incorrect_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name=\"Wiki_Method\"\n",
    "\n",
    "if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "for i in tqdm(range(0,1), desc=\"Processing entries\", unit=\"entry\"):\n",
    "    sentences = preprocess_text(dataset[\"evaluation\"][i][\"gpt3_text\"])\n",
    "    ground_truth = preprocess_text(dataset[\"evaluation\"][i][\"wiki_bio_text\"])\n",
    "    annotation = dataset[\"evaluation\"][i][\"annotation\"]\n",
    "    \n",
    "    sentences_score, sentence_pairs, sentence_links= analyze_text(sentences)\n",
    "    ground_truth_score, ground_pairs, ground_links = analyze_text(ground_truth)\n",
    "    correct_scores.append(ground_truth_score)\n",
    "    print(f\"The ground truth scores are: {ground_truth_score}\\n\")\n",
    "    \n",
    "    correct_count = annotation.count(\"accurate\")\n",
    "    incorrect_count = annotation.count(\"minor_inaccurate\") + annotation.count(\"major_inaccurate\")\n",
    "    total = correct_count + incorrect_count\n",
    "    if(correct_count==total):\n",
    "        correct_scores.append(sentences_score)\n",
    "    if(incorrect_count/total>0.75):\n",
    "        incorrect_scores.append(sentences_score)\n",
    "        \n",
    "    filename = os.path.join(folder_name, f\"entry_{i+1}.txt\")\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(\"#############SENTENCE_PAIRS############\\n\\n\")\n",
    "        for i in range(len(sentence_pairs)):\n",
    "            file.write(f\"{sentence_pairs[i][0][1]} and {sentence_pairs[i][1][1]} and the value is : {sentence_links[i]}\\n\")\n",
    "        file.write(\"\\n\\n\")\n",
    "        file.write(\"#############GROUND_PAIRS############\\n\\n\")\n",
    "        for i in range(len(ground_pairs)):\n",
    "            file.write(f\"{ground_pairs[i][0][1]} and {ground_pairs[i][1][1]} and the value is : {ground_links[i]}\\n\")\n",
    "        file.write(\"\\n\\n\")\n",
    "        file.write(\"%%%%%%%%%%%%%%%%%%%%SENTENCES%%%%%%%%%%%%%%%%%\\n\")\n",
    "        file.write(f\"Sentences : \\n{sentences} \\n\\n\")\n",
    "        file.write(\"\\n\\n\")\n",
    "        file.write(\"%%%%%%%%%%%%%%%%%%%%GROUND_TRUTH%%%%%%%%%%%%%%%\\n\")\n",
    "        file.write(f\"Ground Truth : \\n{ground_truth} \\n\\n\")\n",
    "        file.write(\"\\n\\n\")\n",
    "        file.write(\"%%%%%%%%%%%%%%%%%%%%FRACTIONS%%%%%%%%%%%%%%%%%%\\n\")\n",
    "        file.write(f\"Value for sentences is : {sentences_score} and for ground truth is : {ground_truth_score}\")\n",
    "        file.write(\"\\n\\n\")\n",
    "        file.write(\"%%%%%%%%%%%%%%%%%%%%ANNOTATIONS%%%%%%%%%%%%%%%%\\n\")\n",
    "        for i in annotation:\n",
    "            file.write(f\"{i} \")\n",
    "        file.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(correct_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(incorrect_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5077271430379288\n"
     ]
    }
   ],
   "source": [
    "anvg_coreect_value = 0\n",
    "for val in correct_scores:\n",
    "    anvg_coreect_value+= val\n",
    "anvg_coreect_value/= len(correct_scores)\n",
    "print(anvg_coreect_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31714452101961915\n"
     ]
    }
   ],
   "source": [
    "anvg_incoreect_value = 0\n",
    "count = 0\n",
    "for val in incorrect_scores:\n",
    "    if val>=0:\n",
    "        anvg_incoreect_value+= val\n",
    "        count += 1; \n",
    "anvg_incoreect_value/= count\n",
    "print(anvg_incoreect_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative = 0\n",
    "for val in incorrect_scores:\n",
    "    if val<0:\n",
    "        negative+= 1\n",
    "print(negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(correct_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(incorrect_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "object __array__ method not producing an array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/IPython/core/formatters.py:340\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# Finally look for special method names\u001b[39;00m\n\u001b[1;32m    342\u001b[0m method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/IPython/core/pylabtools.py:152\u001b[0m, in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_bases\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FigureCanvasBase\n\u001b[1;32m    150\u001b[0m     FigureCanvasBase(fig)\n\u001b[0;32m--> 152\u001b[0m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbytes_io\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m data \u001b[38;5;241m=\u001b[39m bytes_io\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fmt \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/matplotlib/backend_bases.py:2218\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2214\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2215\u001b[0m     \u001b[38;5;66;03m# _get_renderer may change the figure dpi (as vector formats\u001b[39;00m\n\u001b[1;32m   2216\u001b[0m     \u001b[38;5;66;03m# force the figure dpi to 72), so we need to set it again here.\u001b[39;00m\n\u001b[1;32m   2217\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m cbook\u001b[38;5;241m.\u001b[39m_setattr_cm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, dpi\u001b[38;5;241m=\u001b[39mdpi):\n\u001b[0;32m-> 2218\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mprint_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2219\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2220\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfacecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfacecolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2221\u001b[0m \u001b[43m            \u001b[49m\u001b[43medgecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medgecolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2222\u001b[0m \u001b[43m            \u001b[49m\u001b[43morientation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morientation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2223\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbbox_inches_restore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_bbox_inches_restore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2224\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2226\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;129;01mand\u001b[39;00m restore_bbox:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/matplotlib/backend_bases.py:2068\u001b[0m, in \u001b[0;36mFigureCanvasBase._switch_canvas_and_return_print_method.<locals>.<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2064\u001b[0m     optional_kws \u001b[38;5;241m=\u001b[39m {  \u001b[38;5;66;03m# Passed by print_figure for other renderers.\u001b[39;00m\n\u001b[1;32m   2065\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdpi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medgecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morientation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2066\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbox_inches_restore\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m   2067\u001b[0m     skip \u001b[38;5;241m=\u001b[39m optional_kws \u001b[38;5;241m-\u001b[39m {\u001b[38;5;241m*\u001b[39minspect\u001b[38;5;241m.\u001b[39msignature(meth)\u001b[38;5;241m.\u001b[39mparameters}\n\u001b[0;32m-> 2068\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mwraps(meth)(\u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2069\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2070\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Let third-parties do as they see fit.\u001b[39;00m\n\u001b[1;32m   2071\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m meth\n",
      "File \u001b[0;32m/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:496\u001b[0m, in \u001b[0;36mFigureCanvasAgg.print_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_png\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, \u001b[38;5;241m*\u001b[39m, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, pil_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    450\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;124;03m    Write the figure to a PNG file.\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;124;03m        *metadata*, including the default 'Software' key.\u001b[39;00m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 496\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_print_pil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpng\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpil_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:444\u001b[0m, in \u001b[0;36mFigureCanvasAgg._print_pil\u001b[0;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_print_pil\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, fmt, pil_kwargs, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    440\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;124;03m    Draw the canvas, then save it using `.image.imsave` (to which\u001b[39;00m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;124;03m    *pil_kwargs* and *metadata* are forwarded).\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 444\u001b[0m     \u001b[43mFigureCanvasAgg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m     mpl\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mimsave(\n\u001b[1;32m    446\u001b[0m         filename_or_obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_rgba(), \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39mfmt, origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupper\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    447\u001b[0m         dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39mdpi, metadata\u001b[38;5;241m=\u001b[39mmetadata, pil_kwargs\u001b[38;5;241m=\u001b[39mpil_kwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:387\u001b[0m, in \u001b[0;36mFigureCanvasAgg.draw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;66;03m# Acquire a lock on the shared font cache.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoolbar\u001b[38;5;241m.\u001b[39m_wait_cursor_for_draw_cm() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoolbar\n\u001b[1;32m    386\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m nullcontext()):\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;66;03m# A GUI class may be need to update a window using this draw, so\u001b[39;00m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;66;03m# don't forget to call the superclass.\u001b[39;00m\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdraw()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/matplotlib/artist.py:95\u001b[0m, in \u001b[0;36m_finalize_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(draw)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw_wrapper\u001b[39m(artist, renderer, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 95\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m renderer\u001b[38;5;241m.\u001b[39m_rasterizing:\n\u001b[1;32m     97\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstop_rasterizing()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/matplotlib/figure.py:3154\u001b[0m, in \u001b[0;36mFigure.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3151\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   3152\u001b[0m         \u001b[38;5;66;03m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[0;32m-> 3154\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3155\u001b[0m mimage\u001b[38;5;241m.\u001b[39m_draw_list_compositing_images(\n\u001b[1;32m   3156\u001b[0m     renderer, \u001b[38;5;28mself\u001b[39m, artists, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppressComposite)\n\u001b[1;32m   3158\u001b[0m renderer\u001b[38;5;241m.\u001b[39mclose_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfigure\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/matplotlib/patches.py:632\u001b[0m, in \u001b[0;36mPatch.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    630\u001b[0m tpath \u001b[38;5;241m=\u001b[39m transform\u001b[38;5;241m.\u001b[39mtransform_path_non_affine(path)\n\u001b[1;32m    631\u001b[0m affine \u001b[38;5;241m=\u001b[39m transform\u001b[38;5;241m.\u001b[39mget_affine()\n\u001b[0;32m--> 632\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_paths_with_artist_properties\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maffine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# Work around a bug in the PDF and SVG renderers, which\u001b[39;49;00m\n\u001b[1;32m    636\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# do not draw the hatches if the facecolor is fully\u001b[39;49;00m\n\u001b[1;32m    637\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# transparent, but do if it is None.\u001b[39;49;00m\n\u001b[1;32m    638\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_facecolor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_facecolor\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/matplotlib/patches.py:617\u001b[0m, in \u001b[0;36mPatch._draw_paths_with_artist_properties\u001b[0;34m(self, renderer, draw_path_args_list)\u001b[0m\n\u001b[1;32m    614\u001b[0m     renderer \u001b[38;5;241m=\u001b[39m PathEffectRenderer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_path_effects(), renderer)\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m draw_path_args \u001b[38;5;129;01min\u001b[39;00m draw_path_args_list:\n\u001b[0;32m--> 617\u001b[0m     \u001b[43mrenderer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdraw_path_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    619\u001b[0m gc\u001b[38;5;241m.\u001b[39mrestore()\n\u001b[1;32m    620\u001b[0m renderer\u001b[38;5;241m.\u001b[39mclose_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/myenv/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:131\u001b[0m, in \u001b[0;36mRendererAgg.draw_path\u001b[0;34m(self, gc, path, transform, rgbFace)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 131\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_renderer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrgbFace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOverflowError\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m         cant_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: object __array__ method not producing an array"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.hist(correct_scores, bins=20, edgecolor='black')\n",
    "plt.xlabel('Correct Scores')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(incorrect_scores, bins=20, edgecolor='black')\n",
    "plt.xlabel('Correct Scores')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_scores_GPT_Only = []\n",
    "incorrect_scores_GPT_Only = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name=\"Only_GPT_Generated\"\n",
    "\n",
    "if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "for i in tqdm(range(130,238), desc=\"Processing entries\", unit=\"entry\"):\n",
    "    sentences = preprocess_text(dataset[\"evaluation\"][i][\"gpt3_text\"])\n",
    "    annotation = dataset[\"evaluation\"][i][\"annotation\"]\n",
    "    \n",
    "    sentences_score, sentence_pairs, sentence_links, sentence_coref_resolved= count_direct_links_and_fraction(sentences)\n",
    "    \n",
    "    correct_count = annotation.count(\"accurate\")\n",
    "    incorrect_count = annotation.count(\"major_inaccurate\")\n",
    "    total = len(annotation)\n",
    "    if(correct_count==total):\n",
    "        correct_scores_GPT_Only.append(sentences_score)\n",
    "    if(incorrect_count/total>0.80):\n",
    "        incorrect_scores_GPT_Only.append(sentences_score)\n",
    "        \n",
    "    filename = os.path.join(folder_name, f\"entry_{i+1}.txt\")\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(\"#############SENTENCE_PAIRS############\\n\\n\")\n",
    "        for i in range(len(sentence_pairs)):\n",
    "            file.write(f\"{sentence_pairs[i][0]} and {sentence_pairs[i][1]} and the value is : {sentence_links[i]}\\n\")\n",
    "        file.write(\"\\n\\n\")\n",
    "        file.write(\"%%%%%%%%%%%%%%%%%%%%SENTENCES%%%%%%%%%%%%%%%%%\\n\")\n",
    "        file.write(f\"Sentences : \\n{sentences} \\n\\n\")\n",
    "        file.write(f\"Coref Resolved : \\n{sentence_coref_resolved}\")\n",
    "        file.write(\"\\n\\n\")\n",
    "        file.write(\"%%%%%%%%%%%%%%%%%%%%FRACTIONS%%%%%%%%%%%%%%%%%%\\n\")\n",
    "        file.write(f\"Value for sentences is : {sentences_score}\")\n",
    "        file.write(\"\\n\\n\")\n",
    "        file.write(\"%%%%%%%%%%%%%%%%%%%%ANNOTATIONS%%%%%%%%%%%%%%%%\\n\")\n",
    "        for i in annotation:\n",
    "            file.write(f\"{i} \")\n",
    "        file.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(correct_scores_GPT_Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(incorrect_scores_GPT_Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(correct_scores_GPT_Only)/len(correct_scores_GPT_Only))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(incorrect_scores_GPT_Only)/len(incorrect_scores_GPT_Only))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentation GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name=\"Name_reduction_check\"\n",
    "\n",
    "if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "for i in tqdm(range(0,3), desc=\"Processing entries\", unit=\"entry\"):\n",
    "    sentences = preprocess_text(dataset[\"evaluation\"][i][\"gpt3_text\"])\n",
    "    ground_truth = preprocess_text(dataset[\"evaluation\"][i][\"wiki_bio_text\"])\n",
    "    annotation = dataset[\"evaluation\"][i][\"annotation\"]\n",
    "    \n",
    "    \n",
    "    correct_scores.append(ground_truth_score)\n",
    "    print(f\"The ground truth scores are: {ground_truth_score}\\n\")\n",
    "    \n",
    "    correct_count = annotation.count(\"accurate\")\n",
    "    incorrect_count = annotation.count(\"minor_inaccurate\") + annotation.count(\"major_inaccurate\")\n",
    "    total = correct_count + incorrect_count\n",
    "    if(correct_count==total):\n",
    "        correct_scores.append(sentences_score)\n",
    "    if(incorrect_count/total>0.75):\n",
    "        incorrect_scores.append(sentences_score)\n",
    "        \n",
    "    filename = os.path.join(folder_name, f\"entry_{i+1}.txt\")\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(\"#############SENTENCE_PAIRS############\\n\\n\")\n",
    "        for i in range(len(sentence_pairs)):\n",
    "            file.write(f\"{sentence_pairs[i][0]} and {sentence_pairs[i][1]} and the value is : {sentence_links[i]}\\n\")\n",
    "        file.write(\"\\n\\n\")\n",
    "        file.write(\"#############GROUND_PAIRS############\\n\\n\")\n",
    "        for i in range(len(ground_pairs)):\n",
    "            file.write(f\"{ground_pairs[i][0]} and {ground_pairs[i][1]} and the value is : {ground_links[i]}\\n\")\n",
    "        file.write(\"\\n\\n\")\n",
    "        file.write(\"%%%%%%%%%%%%%%%%%%%%SENTENCES%%%%%%%%%%%%%%%%%\\n\")\n",
    "        file.write(f\"Sentences : \\n{sentences} \\n\\n\")\n",
    "        file.write(f\"Coref Resolved : \\n{sentence_coref_resolved}\")\n",
    "        file.write(\"\\n\\n\")\n",
    "        file.write(\"%%%%%%%%%%%%%%%%%%%%GROUND_TRUTH%%%%%%%%%%%%%%%\\n\")\n",
    "        file.write(f\"Ground Truth : \\n{ground_truth} \\n\\n\")\n",
    "        file.write(f\"Coref Resolved : \\n{ground_coref_resolved}\")\n",
    "        file.write(\"\\n\\n\")\n",
    "        file.write(\"%%%%%%%%%%%%%%%%%%%%FRACTIONS%%%%%%%%%%%%%%%%%%\\n\")\n",
    "        file.write(f\"Value for sentences is : {sentences_score} and for ground truth is : {ground_truth_score}\")\n",
    "        file.write(\"\\n\\n\")\n",
    "        file.write(\"%%%%%%%%%%%%%%%%%%%%ANNOTATIONS%%%%%%%%%%%%%%%%\\n\")\n",
    "        for i in annotation:\n",
    "            file.write(f\"{i} \")\n",
    "        file.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings based Name Extraction (Linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_entity(uri, label):\n",
    "        file.write(f\"URI: {uri}, Label: {label}\\n\")\n",
    "        print(f\"URI: {uri}, Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DBPedia_Information = []\n",
    "DBpedia_Embeddings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "limit = 500\n",
    "offset = 0\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        query = f\"\"\"\n",
    "        SELECT ?entity ?label ?abstract WHERE {{\n",
    "          ?entity a dbo:Person .\n",
    "          OPTIONAL {{ ?entity rdfs:label ?label . FILTER (lang(?label) = \"en\") }}\n",
    "          OPTIONAL {{ ?entity dbo:abstract ?abstract . FILTER (lang(?abstract) = \"en\") }}\n",
    "        }}\n",
    "        LIMIT {limit} OFFSET {offset}\n",
    "        \"\"\"\n",
    "        \n",
    "        sparql.setQuery(query)\n",
    "        sparql.setReturnFormat(JSON)\n",
    "        \n",
    "        results = sparql.query().convert()\n",
    "        \n",
    "        bindings = results[\"results\"][\"bindings\"]\n",
    "        if not bindings:\n",
    "            break\n",
    "        \n",
    "        for result in bindings:\n",
    "            entry = {\n",
    "                \"uri\": result[\"entity\"][\"value\"],\n",
    "                \"label\": result[\"label\"][\"value\"] if \"label\" in result else \"\",\n",
    "                \"abstract\": result[\"abstract\"][\"value\"] if \"abstract\" in result else \"\"\n",
    "            }\n",
    "            DBPedia_Information.append(entry)\n",
    "            print(entry)\n",
    "            context = entry[\"label\"] + \": \" + entry[\"abstract\"]\n",
    "            current_embedding = model.encode(context,show_progress_bar=False)\n",
    "            DBpedia_Embeddings.append(current_embedding)  \n",
    "        \n",
    "        offset += limit\n",
    "              \n",
    "    except Exception as e:\n",
    "        print(f\"Error while creating embeddings/extracting entities: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARALLEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_entities(offset, limit):\n",
    "    sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "    query = f\"\"\"\n",
    "    SELECT ?entity ?label ?abstract WHERE {{\n",
    "        ?entity a dbo:Person .\n",
    "        OPTIONAL {{ ?entity rdfs:label ?label . FILTER (lang(?label) = \"en\") }}\n",
    "        OPTIONAL {{ ?entity dbo:abstract ?abstract . FILTER (lang(?abstract) = \"en\") }}\n",
    "    }}\n",
    "    LIMIT {limit} OFFSET {offset}\n",
    "    \"\"\"\n",
    "    \n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    \n",
    "    results = sparql.query().convert()\n",
    "    \n",
    "    fetched_data = []\n",
    "    for result in results[\"results\"][\"bindings\"]:\n",
    "        label = result[\"label\"][\"value\"] if \"label\" in result else \"\"\n",
    "        abstract = result[\"abstract\"][\"value\"] if \"abstract\" in result else \"\"\n",
    "        context = label + \": \" + abstract\n",
    "        current_embedding = model.encode(context, show_progress_bar=False)\n",
    "        \n",
    "        entry = {\n",
    "            \"uri\": result[\"entity\"][\"value\"],\n",
    "            \"label\": result[\"label\"][\"value\"] if \"label\" in result else \"\",\n",
    "            \"abstract\": result[\"abstract\"][\"value\"] if \"abstract\" in result else \"\",\n",
    "            \"embedding\": current_embedding\n",
    "        }\n",
    "        fetched_data.append(entry)\n",
    "    return fetched_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DBpedia_Database= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 100  # Adjust the batch size as needed\n",
    "total_entities = 2200000  # Total number of entities to fetch\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    # Calculate offsets for pagination\n",
    "    offsets = range(0, total_entities, limit)\n",
    "    \n",
    "    # Submit tasks for each offset to fetch entities\n",
    "    future_to_offset = {executor.submit(fetch_entities, offset, limit): offset for offset in offsets}\n",
    "    \n",
    "    for future in concurrent.futures.as_completed(future_to_offset):\n",
    "        try:\n",
    "            entities = future.result()\n",
    "            # Extend DBpedia_Database with fetched entities\n",
    "            DBpedia_Database.extend(entities)\n",
    "            # Process fetched entities as needed\n",
    "            for entity in entities:\n",
    "                # Access entity[\"uri\"], entity[\"label\"], entity[\"abstract\"], entity[\"embedding\"]\n",
    "                print(entity[\"uri\"], entity[\"label\"], entity[\"abstract\"], entity[\"embedding\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching entities: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(DBPedia_Information))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "embeddings = np.random.rand(2300000, 768).astype('float32')\n",
    "names = ['Embedding_' + str(i) for i in range(2300000)] \n",
    "\n",
    "index = faiss.IndexFlatL2(768)\n",
    "\n",
    "index.add(embeddings)\n",
    "\n",
    "query_embedding = np.random.rand(1, 768).astype('float32')  \n",
    "k = 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=1\n",
    "distances, indices = index.search(query_embedding, k)\n",
    "\n",
    "# Step 4: Display results\n",
    "print(\"Query Embedding:\")\n",
    "print(query_embedding)\n",
    "print(\"\\nNearest Neighbors:\")\n",
    "for i in range(k):\n",
    "    print(f\"Name: {names[indices[0][i]]}, Distance: {distances[0][i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DBpedia_Entities = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "sparql.setReturnFormat(JSON)\n",
    "\n",
    "offset = 298000\n",
    "limit = 2000\n",
    "\n",
    "total_entities = 2293328  # Total number of entities in DBpedia\n",
    "progress_bar = tqdm(total=total_entities, desc=\"Fetching Entities\", unit=\" entities\")\n",
    "\n",
    "# Open a text file for writing\n",
    "with open('dbpedia_entities.txt', 'a', encoding='utf-8') as file:\n",
    "    while True:\n",
    "        # Set the SPARQL query with limit and offset\n",
    "        query = f\"\"\"\n",
    "        SELECT DISTINCT ?entity ?label (COALESCE(?abstract_en, \"\") AS ?abstract) WHERE {{\n",
    "          ?entity a dbo:Person .\n",
    "          OPTIONAL {{ \n",
    "            ?entity rdfs:label ?label .\n",
    "            FILTER(LANG(?label) = 'en')\n",
    "          }}\n",
    "          OPTIONAL {{ \n",
    "            ?entity dbo:abstract ?abstract_en .\n",
    "            FILTER(LANG(?abstract_en) = 'en')\n",
    "          }}\n",
    "        }}\n",
    "        LIMIT {limit} OFFSET {offset}\n",
    "        \"\"\"\n",
    "        \n",
    "        sparql.setQuery(query)\n",
    "        \n",
    "        # Execute the query\n",
    "        results = sparql.query().convert()\n",
    "        \n",
    "        # Check if results are returned\n",
    "        if len(results[\"results\"][\"bindings\"]) == 0:\n",
    "            break  # No more results\n",
    "        \n",
    "        # Process each result and write to the file\n",
    "        for result in results[\"results\"][\"bindings\"]:\n",
    "            entity_uri = result[\"entity\"][\"value\"]\n",
    "            label = result[\"label\"][\"value\"] if \"label\" in result else \"\"\n",
    "            abstract = result[\"abstract\"][\"value\"] if \"abstract\" in result else \"\"\n",
    "            \n",
    "            # Write entity information to the file\n",
    "            file.write(f\"Entity URI: {entity_uri}\\n\")\n",
    "            file.write(f\"Label: {label}\\n\")\n",
    "            file.write(f\"Abstract: {abstract}\\n\")\n",
    "            file.write(\"\\n\")\n",
    "            \n",
    "            # Create dictionary for the entry\n",
    "            entry = {\n",
    "                \"entity_uri\": entity_uri,\n",
    "                \"label\": label,\n",
    "                \"abstract\": abstract\n",
    "            }\n",
    "            \n",
    "            # Append entry to the list\n",
    "            DBpedia_Entities.append(entry)\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.update(1)\n",
    "        \n",
    "        # Increment the offset for the next iteration\n",
    "        offset += limit\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DBpedia_Entities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
